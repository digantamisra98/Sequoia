import sys
from dataclasses import dataclass
from typing import Dict, NamedTuple, Optional, Tuple

import gym
import numpy as np
import torch
import tqdm
from gym import Space, spaces
from numpy import inf
from simple_parsing import ArgumentParser
from torch import Tensor
from copy import deepcopy

from sequoia.methods import register_method
from sequoia.common import Config
from sequoia.common.spaces import Image
from sequoia.settings import Method
from sequoia.settings.passive import TaskIncrementalSetting
from sequoia.settings.passive.cl.objects import (Actions, Observations,
                                                 PassiveEnvironment, Rewards)



class LWFNet(torch.nn.Module):

    def __init__(self, image_space: Image, n_classes_per_task: Dict[int, int]):
        super().__init__()

        ncha = image_space.channels
        size = image_space.width
        self.n_classes_per_task = n_classes_per_task

        self.c1 = torch.nn.Conv2d(ncha, 64, kernel_size=size // 8)
        s = compute_conv_output_size(size, size // 8)
        s //= 2
        self.c2 = torch.nn.Conv2d(64, 128, kernel_size=size // 10)
        s = compute_conv_output_size(s, size // 10)
        s //= 2
        self.c3 = torch.nn.Conv2d(128, 256, kernel_size=2)
        s = compute_conv_output_size(s, 2)
        s //= 2
        self.smid = s
        self.maxpool = torch.nn.MaxPool2d(2)
        self.relu = torch.nn.ReLU()

        self.drop1 = torch.nn.Dropout(0.2)
        self.drop2 = torch.nn.Dropout(0.5)
        self.fc1 = torch.nn.Linear(256 * self.smid * self.smid, 2048)
        self.fc2 = torch.nn.Linear(2048, 2048)
        self.output_layers = torch.nn.ModuleList()

        for t, n in self.n_classes_per_task.items():
            self.output_layers.append(torch.nn.Linear(2048, n))

        self.loss = torch.nn.CrossEntropyLoss()
        self.current_task: Optional[int] = 0
        self.flatten = torch.nn.Flatten()

    def forward(self, observations: TaskIncrementalSetting.Observations):
        x = observations.x
        h = self.maxpool(self.drop1(self.relu(self.c1(x))))
        h = self.maxpool(self.drop1(self.relu(self.c2(h))))
        h = self.maxpool(self.drop2(self.relu(self.c3(h))))
        h = self.flatten(h)
        h = self.drop2(self.relu(self.fc1(h)))
        h = self.drop2(self.relu(self.fc2(h)))

        # Each batch can have elements of more than one Task (in test)
        # In Task Incremental Learning, each task have it own classification head.
        y = []
        for t, n in self.n_classes_per_task.items():
            y.append(self.output_layers[t](h))

        return y


def compute_conv_output_size(Lin: int, kernel_size: int, stride: int = 1, padding: int = 0, dilation: int = 1) -> int:
    return int(np.floor((Lin + 2 * padding - dilation * (kernel_size - 1) - 1) / float(stride) + 1))


@register_method
class LWFMethod(Method, target_setting=TaskIncrementalSetting):
    """
    Here we implement the method according to the characteristics and methodology of the current proposal.
    It should be as much as possible agnostic to the model and setting we are going to use.

    The method proposed can be specific to a setting to make comparisons easier.
    Here what we control is the model's training process, given a setting that delivers data in a certain way.
    """

    @dataclass
    class HParams:
        """ Hyper-parameters of the Settings. """
        # Learning rate of the optimizer.
        learning_rate: float = 0.001
        # Batch size
        batch_size: int = 128
        # Importance of KD loss
        lamb: float = 2
        # Scale parameter
        temperature: float = 2
        # Maximum number of training epochs per task
        max_epochs_per_task: int = 2

    def __init__(self, hparams: HParams = None):
        self.hparams: LWFMethod.HParams = hparams or self.HParams()
        self.ce_criterion = torch.nn.CrossEntropyLoss()
        # We will create those when `configure` will be called, before training.
        self.model: LWFNet
        # Define old model
        self.model_old = None
        self.optimizer: torch.optim.Optimizer

    def configure(self, setting: TaskIncrementalSetting):
        """ Called before the method is applied on a setting (before training).

        You can use this to instantiate your model, for instance, since this is
        where you get access to the observation & action spaces.
        """
        setting.batch_size = self.hparams.batch_size
        assert setting.increment == setting.test_increment, (
            "Assuming same number of classes per task for training and testing."
        )
        n_classes_per_task = {
            i: setting.num_classes_in_task(i, train=True)
            for i in range(setting.nb_tasks)
        }
        image_space: Image = setting.observation_space[0]
        self.model = LWFNet(
            image_space=image_space,
            n_classes_per_task=n_classes_per_task
        )
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.learning_rate)

    def freeze_model(self, model):
        for param in model.parameters():
            param.requires_grad = False
        return

    def fit(self, train_env: PassiveEnvironment, valid_env: PassiveEnvironment):
        """
        Train loop

        Different Settings can return elements from tasks in an other  way,
        be it class incremental, task incremental, etc.

        Batch can have information about en environment, rewards, input, task labels, etc.
        And we call the forward training function of our method, independent of the settings
        """
        # configure() will have been called by the setting before we get here,

        best_val_loss = inf
        best_epoch = 0
        for epoch in range(self.hparams.max_epochs_per_task):
            self.model.train()
            print(f"Starting epoch {epoch}")
            # Training loop:
            with tqdm.tqdm(train_env) as train_pbar:
                postfix = {}
                train_pbar.set_description(f"Training Epoch {epoch}")
                for i, batch in enumerate(train_pbar):
                    loss, metrics_dict = self.shared_step(batch)
                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()
                    postfix.update(metrics_dict)
                    train_pbar.set_postfix(postfix)

            # Validation loop:
            self.model.eval()
            torch.set_grad_enabled(False)
            with tqdm.tqdm(valid_env) as val_pbar:
                postfix = {}
                val_pbar.set_description(f"Validation Epoch {epoch}")
                epoch_val_loss = 0.

                for i, batch in enumerate(val_pbar):
                    batch_val_loss, metrics_dict = self.shared_step(batch)
                    epoch_val_loss += batch_val_loss
                    postfix.update(metrics_dict, val_loss=epoch_val_loss)
                    val_pbar.set_postfix(postfix)
            torch.set_grad_enabled(True)

            if epoch_val_loss < best_val_loss:
                best_val_loss = epoch_val_loss
                best_epoch = i

        # Restore current model as old
        self.model_old = deepcopy(self.model)
        self.model_old.eval()
        self.freeze_model(self.model_old)

    ## Move shared-step from "Model" to "Method"
    def shared_step(self, batch: Tuple[Observations, Rewards]) -> Tuple[Tensor, Dict]:
        observations: Observations = batch[0]
        rewards: Rewards = batch[1]
        image_labels = rewards.y

        # Get the predictions from new model:
        logits = self.model.forward(observations)
        _, y_pred = logits[self.model.current_task].max(1)

        # Get the predictions from old model:
        logits_old = None
        if  self.model.current_task > 0:
            logits_old = self.model_old.forward(observations)

        loss = self.criterion(logits_old, logits, image_labels)

        accuracy = (y_pred == image_labels).sum().float() / len(image_labels)
        metrics_dict = {"accuracy": accuracy}
        return loss, metrics_dict

    def criterion(self, logits_old, logits, labels):
        t = self.model.current_task
        # Cross entropy loss
        loss_ce = self.ce_criterion(logits[t], labels)

        # KD loss
        loss_kd = 0
        for t_old in range(0, t):
            soft_target = torch.nn.functional.softmax(logits_old[t] / self.hparams.temperature, dim=1)
            logp = torch.nn.functional.log_softmax(logits[t] / self.hparams.temperature, dim=1)
            loss_kd += -torch.mean(torch.sum(soft_target * logp, dim=1))

        # Total loss
        loss_total = loss_ce + self.hparams.lamb * loss_kd

        return loss_total

    def get_actions(self, observations: Observations, action_space: gym.Space) -> Actions:
        """ Get a batch of predictions (aka actions) for these observations. """
        with torch.no_grad():
            logits = self.model.forward(observations)
        # Get the predicted classes
        _, y_pred = logits[self.model.current_task].max(1)
        return self.target_setting.Actions(y_pred)

    def on_task_switch(self, task_id: Optional[int]):
        # This method gets called if task boundaries are known in the current
        # setting. Furthermore, if task labels are available, task_id will be
        # the index of the new task. If not, task_id will be None.
        # TODO: Does this method actually work when task_id is None?
        self.model.current_task = task_id


if __name__ == "__main__":
    # # Example: Evaluate a Method on a single CL setting:
    # parser = ArgumentParser(description=__doc__, add_dest_to_option_strings=False)
    #
    # """
    # We must define 3 main components:
    #  1.- Setting: It is the continual learning scenario that we are working, SL or RL, TI or CI
    #               Each settings has it own parameters that can be customized.
    #  2.- Model: Is the parameters and layers of the model, just like in PyTorch.
    #             We can use a predefined model or create your own
    #  3.- Method: It is how we are going to use what the settings give us to train our model.
    #              Same as before, we can define our own or use pre-defined Methods.
    # """
    # ## Add arguments for the Method, the Setting, and the Config.
    # ## (Config contains options like the log_dir, the data_dir, etc.)
    # HatMethod.add_argparse_args(parser, dest="method")
    # parser.add_arguments(TaskIncrementalSetting, dest="setting")
    # parser.add_arguments(Config, "config")
    #
    # args = parser.parse_args()
    #
    # ## Create the Method from the args, and extract the Setting, and the Config:
    # method: HatMethod = HatMethod.from_argparse_args(args, dest="method")
    # setting: TaskIncrementalSetting = args.setting
    # config: Config = args.config
    #
    # ## Apply the method to the setting, optionally passing in a Config,
    # ## producing Results.
    # results = setting.apply(method, config=config)
    # print(results.summary())
    # print(f"objective: {results.objective}")

    ## 1. Creating the setting:
    setting = TaskIncrementalSetting(dataset="mnist", batch_size=32)
    ## 2. Creating the Method
    method = LWFMethod()
    # (Optional): You can also create a Config, which holds other fields like
    # `log_dir`, `debug`, `device`, etc. which aren't specific to either the
    # Setting or the Method.
    config = Config()
    ## 3. Applying the method to the setting: (optionally passing a Config to
    # use for that run)
    results = setting.apply(method, config=config)
    print(results.summary())
    print(f"objective: {results.objective}")
